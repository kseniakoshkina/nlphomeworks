{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Домашнее задание №1 студентки группы 181 Кошкиной Ксении"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from pymorphy2.tokenizers import simple_word_tokenize\n",
    "import RAKE\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from summa import keywords\n",
    "import yake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Тексты взяты с сайта vesti.ru, где внизу каждой статьи прилагаются ключевые слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36'\n",
    "    }\n",
    "    resp = requests.get(url, headers=headers)\n",
    "    html = resp.text\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df['source'] = ['https://www.vesti.ru/article/2635384', 'https://www.vesti.ru/hitech/article/2635408', \\\n",
    "                    'https://www.vesti.ru/article/2635790', 'https://www.vesti.ru/article/2635756', \\\n",
    "                    'https://www.vesti.ru/article/2635786', 'https://www.vesti.ru/article/2635754', \\\n",
    "                    'https://www.vesti.ru/article/2635941', 'https://www.vesti.ru/article/2636008', \\\n",
    "                     'https://www.vesti.ru/article/2635988']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df['text'] = main_df['source'].apply(get_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Выделеннные мной ключевые слова:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_keywords1 = ['Милош Земан', 'президент', 'реанимация', 'состояние здоровья', 'правительство', 'Чехия', 'военный госпиталь', 'президентские полномочия']\n",
    "my_keywords2 = ['деньги', 'мошенники', 'атака', 'жертва мошенников', 'сообщение', 'банковская карта', 'код доступа']\n",
    "my_keywords3 = ['Аэрофлот', 'Пхукет', 'Таиланд', 'регулярное авиасообщение', 'привитые россияне', 'туристы', 'Бангкок', 'Паттайя']\n",
    "my_keywords4 = ['Глазго', 'президент', 'Байден', 'климатическая конференция', 'глобальное потепление','углексислый газ', 'изменение климата']\n",
    "my_keywords5 = ['Томас Бенфилд', 'вакцинация', 'коронавирус', 'симптомы', 'снижение риска заболевания', 'течение болезни']\n",
    "my_keywords6 = ['Еврейская автономная область', 'прививка', 'вакцинация', 'увольнение', 'работники скорой помощи', 'уходящие медики', 'заявление на увольнение', 'Облучье']\n",
    "my_keywords7 = ['Новые Ватутинки', 'инцидент', 'следствие', 'драка', 'Александр Жиловников', 'Никита Уткин', 'полиция', 'злоумышленники']\n",
    "my_keywords8 = ['официальное место работы', 'полиция', 'нападение', 'Новая Москва', 'уроженцы Кавказа', 'Новые Ватутинки', 'маленький сын', 'нож']\n",
    "my_keywords9 = ['Джон Кеннеди', 'Ли Харви Освальд', 'конспирологи', 'теория заговора', 'убийство Кеннеди', 'спасение Америки', 'торговля детьми', 'Дональд Трамп', 'появление Кеннеди']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df['my_keywords'] = [my_keywords1, my_keywords2, my_keywords3, my_keywords4, my_keywords5, \\\n",
    "                         my_keywords6, my_keywords7, my_keywords8, my_keywords9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Функция для того, чтобы достать первичные ключевые слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords(text):\n",
    "    soup = BeautifulSoup(text,'html.parser')\n",
    "    finalData = []\n",
    "    for i in soup.find_all('div', {'class' :'tags'}):\n",
    "        for element in str(i).split('>'):\n",
    "            f = re.findall(r'[А-Я]*[а-я]*', element)\n",
    "            keywords = list(filter(None, f))\n",
    "            if len(keywords) != 0:\n",
    "                finalData.append(' '.join(keywords))\n",
    "    return finalData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df['keywords_from_source'] = main_df['text'].apply(get_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>my_keywords</th>\n",
       "      <th>keywords_from_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.vesti.ru/article/2635384</td>\n",
       "      <td>&lt;!doctype html&gt;\\n&lt;html lang=\"ru\"&gt;\\n\\n    &lt;head...</td>\n",
       "      <td>[Милош Земан, президент, реанимация, состояние...</td>\n",
       "      <td>[общество, в мире, политика, президент, Чехия,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.vesti.ru/hitech/article/2635408</td>\n",
       "      <td>&lt;!doctype html&gt;\\n&lt;html lang=\"ru\"&gt;\\n\\n    &lt;head...</td>\n",
       "      <td>[деньги, мошенники, атака, жертва мошенников, ...</td>\n",
       "      <td>[общество, происшествия, технологии, обман, об...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.vesti.ru/article/2635790</td>\n",
       "      <td>&lt;!doctype html&gt;\\n&lt;html lang=\"ru\"&gt;\\n\\n    &lt;head...</td>\n",
       "      <td>[Аэрофлот, Пхукет, Таиланд, регулярное авиасоо...</td>\n",
       "      <td>[экономика, Таиланд, Аэрофлот, Бангкок, Пхукет...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.vesti.ru/article/2635756</td>\n",
       "      <td>&lt;!doctype html&gt;\\n&lt;html lang=\"ru\"&gt;\\n\\n    &lt;head...</td>\n",
       "      <td>[Глазго, президент, Байден, климатическая конф...</td>\n",
       "      <td>[общество, в мире, встреча, климат, США Америк...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.vesti.ru/article/2635786</td>\n",
       "      <td>&lt;!doctype html&gt;\\n&lt;html lang=\"ru\"&gt;\\n\\n    &lt;head...</td>\n",
       "      <td>[Томас Бенфилд, вакцинация, коронавирус, симпт...</td>\n",
       "      <td>[общество, медицина, болезнь, Дания, вакцинаци...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://www.vesti.ru/article/2635754</td>\n",
       "      <td>&lt;!doctype html&gt;\\n&lt;html lang=\"ru\"&gt;\\n\\n    &lt;head...</td>\n",
       "      <td>[Еврейская автономная область, прививка, вакци...</td>\n",
       "      <td>[общество, медицина, скорая помощь, увольнение...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://www.vesti.ru/article/2635941</td>\n",
       "      <td>&lt;!doctype html&gt;\\n&lt;html lang=\"ru\"&gt;\\n\\n    &lt;head...</td>\n",
       "      <td>[Новые Ватутинки, инцидент, следствие, драка, ...</td>\n",
       "      <td>[общество, происшествия, драка, Новая Москва, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://www.vesti.ru/article/2636008</td>\n",
       "      <td>&lt;!doctype html&gt;\\n&lt;html lang=\"ru\"&gt;\\n\\n    &lt;head...</td>\n",
       "      <td>[официальное место работы, полиция, нападение,...</td>\n",
       "      <td>[происшествия, нападение, избиение, Новая Моск...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://www.vesti.ru/article/2635988</td>\n",
       "      <td>&lt;!doctype html&gt;\\n&lt;html lang=\"ru\"&gt;\\n\\n    &lt;head...</td>\n",
       "      <td>[Джон Кеннеди, Ли Харви Освальд, конспирологи,...</td>\n",
       "      <td>[общество, политика, секта, США Америка, консп...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        source  \\\n",
       "0         https://www.vesti.ru/article/2635384   \n",
       "1  https://www.vesti.ru/hitech/article/2635408   \n",
       "2         https://www.vesti.ru/article/2635790   \n",
       "3         https://www.vesti.ru/article/2635756   \n",
       "4         https://www.vesti.ru/article/2635786   \n",
       "5         https://www.vesti.ru/article/2635754   \n",
       "6         https://www.vesti.ru/article/2635941   \n",
       "7         https://www.vesti.ru/article/2636008   \n",
       "8         https://www.vesti.ru/article/2635988   \n",
       "\n",
       "                                                text  \\\n",
       "0  <!doctype html>\\n<html lang=\"ru\">\\n\\n    <head...   \n",
       "1  <!doctype html>\\n<html lang=\"ru\">\\n\\n    <head...   \n",
       "2  <!doctype html>\\n<html lang=\"ru\">\\n\\n    <head...   \n",
       "3  <!doctype html>\\n<html lang=\"ru\">\\n\\n    <head...   \n",
       "4  <!doctype html>\\n<html lang=\"ru\">\\n\\n    <head...   \n",
       "5  <!doctype html>\\n<html lang=\"ru\">\\n\\n    <head...   \n",
       "6  <!doctype html>\\n<html lang=\"ru\">\\n\\n    <head...   \n",
       "7  <!doctype html>\\n<html lang=\"ru\">\\n\\n    <head...   \n",
       "8  <!doctype html>\\n<html lang=\"ru\">\\n\\n    <head...   \n",
       "\n",
       "                                         my_keywords  \\\n",
       "0  [Милош Земан, президент, реанимация, состояние...   \n",
       "1  [деньги, мошенники, атака, жертва мошенников, ...   \n",
       "2  [Аэрофлот, Пхукет, Таиланд, регулярное авиасоо...   \n",
       "3  [Глазго, президент, Байден, климатическая конф...   \n",
       "4  [Томас Бенфилд, вакцинация, коронавирус, симпт...   \n",
       "5  [Еврейская автономная область, прививка, вакци...   \n",
       "6  [Новые Ватутинки, инцидент, следствие, драка, ...   \n",
       "7  [официальное место работы, полиция, нападение,...   \n",
       "8  [Джон Кеннеди, Ли Харви Освальд, конспирологи,...   \n",
       "\n",
       "                                keywords_from_source  \n",
       "0  [общество, в мире, политика, президент, Чехия,...  \n",
       "1  [общество, происшествия, технологии, обман, об...  \n",
       "2  [экономика, Таиланд, Аэрофлот, Бангкок, Пхукет...  \n",
       "3  [общество, в мире, встреча, климат, США Америк...  \n",
       "4  [общество, медицина, болезнь, Дания, вакцинаци...  \n",
       "5  [общество, медицина, скорая помощь, увольнение...  \n",
       "6  [общество, происшествия, драка, Новая Москва, ...  \n",
       "7  [происшествия, нападение, избиение, Новая Моск...  \n",
       "8  [общество, политика, секта, США Америка, консп...  "
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Извлекаем из html кода чистый текст:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_text(text):\n",
    "    data = []\n",
    "    soup = BeautifulSoup(text,'html.parser')\n",
    "    for i in soup.find_all('div', {'class' :'article__text'}):\n",
    "        f = re.findall(r'[А-Я]*[а-я]*', str(i))\n",
    "        for item in f:\n",
    "            if len(item) != 0:\n",
    "                data.append(item)\n",
    "    return ' '.join(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df['clean_text'] = main_df['text'].apply(get_clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df = main_df.drop(('text'),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Функция для токенизации текста, взятая из тетрадки семинара:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MorphAnalyzer()\n",
    "def normalize_text(text):\n",
    "    lemmas = []\n",
    "    for t in simple_word_tokenize(text):\n",
    "        lemmas.append(\n",
    "            m.parse(t)[0].normal_form\n",
    "        )\n",
    "    return ' '.join(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_stopwords = stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "rake = RAKE.Rake(list_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df['normalize_text'] = main_df['clean_text'].apply(normalize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df['rake'] = main_df['normalize_text'].apply(lambda x:\n",
    "                                                  rake.run(x, maxWords=3, minFrequency=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df['textrank'] = main_df['normalize_text'].apply(lambda x:\n",
    "                                                     keywords.keywords(x, language='russian', additional_stopwords=list_stopwords, scores=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_extractor = yake.KeywordExtractor()\n",
    "max_ngram_size = 2\n",
    "deduplication_threshold = 0.9\n",
    "numOfKeywords = 10\n",
    "custom_kw_extractor = yake.KeywordExtractor(lan='russian', n=max_ngram_size, dedupLim=deduplication_threshold, top=numOfKeywords, features=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df['yake'] = main_df['normalize_text'].apply(lambda x: custom_kw_extractor.extract_keywords(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Объединим ключевые слова из статьи и ключевые слова, размеченные мной, и удалим какие-то общие тэги"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenation(my_keywords, keywords_from_source):\n",
    "    list_of_keywords = []\n",
    "    for i in range(len(my_keywords)):\n",
    "        final_words = list(set(my_keywords[i] + keywords_from_source[i]))\n",
    "        if 'общество' in final_words:\n",
    "            final_words.remove('общество')\n",
    "        if 'в мире' in final_words:\n",
    "            final_words.remove('в мире')\n",
    "        if 'новости' in final_words:\n",
    "            final_words.remove('новости')\n",
    "        else:\n",
    "            pass\n",
    "        list_of_keywords.append(final_words)\n",
    "    return list_of_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df['combined_keywords'] = concatenation(list(main_df.my_keywords), list(main_df.keywords_from_source))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>my_keywords</th>\n",
       "      <th>keywords_from_source</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>normalize_text</th>\n",
       "      <th>rake</th>\n",
       "      <th>textrank</th>\n",
       "      <th>yake</th>\n",
       "      <th>combined_keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.vesti.ru/article/2635384</td>\n",
       "      <td>[Милош Земан, президент, реанимация, состояние...</td>\n",
       "      <td>[общество, в мире, политика, президент, Чехия,...</td>\n",
       "      <td>Медицинский консилиум оценит в пятницу состоян...</td>\n",
       "      <td>медицинский консилиум оценить в пятница состоя...</td>\n",
       "      <td>[(медицинский консилиум оценить, 9.0), (состоя...</td>\n",
       "      <td>[(земан, 0.1923731054306261), (состояние здоро...</td>\n",
       "      <td>[(состояние здоровье, 0.0005116003525649392), ...</td>\n",
       "      <td>[правительство, состояние здоровья, президент,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.vesti.ru/hitech/article/2635408</td>\n",
       "      <td>[деньги, мошенники, атака, жертва мошенников, ...</td>\n",
       "      <td>[общество, происшествия, технологии, обман, об...</td>\n",
       "      <td>Нередко жертвами мошенников становятся те кто ...</td>\n",
       "      <td>нередко жертва мошенник становиться тот кто ра...</td>\n",
       "      <td>[(якобы проверить действительно, 9.0), (сайт м...</td>\n",
       "      <td>[(атака, 0.21545317808030248), (жертва мошенни...</td>\n",
       "      <td>[(виктор чебышев, 0.002687649831204984), (касп...</td>\n",
       "      <td>[мошенник, обман, технологии, жертва мошеннико...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.vesti.ru/article/2635790</td>\n",
       "      <td>[Аэрофлот, Пхукет, Таиланд, регулярное авиасоо...</td>\n",
       "      <td>[экономика, Таиланд, Аэрофлот, Бангкок, Пхукет...</td>\n",
       "      <td>Первый за последние месяцев самолет компании А...</td>\n",
       "      <td>один за последний месяц самолёт компания аэроф...</td>\n",
       "      <td>[(выполнять регулярный авиарейс, 9.0), (прохож...</td>\n",
       "      <td>[(таиланд, 0.22954872965166995), (регулярный, ...</td>\n",
       "      <td>[(составлять миллион, 0.008638577397523413), (...</td>\n",
       "      <td>[Аэрофлот, Паттайя, Пхукет, Таиланд, экономика...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.vesti.ru/article/2635756</td>\n",
       "      <td>[Глазго, президент, Байден, климатическая конф...</td>\n",
       "      <td>[общество, в мире, встреча, климат, США Америк...</td>\n",
       "      <td>Климатическая конференция которая должна была ...</td>\n",
       "      <td>климатический конференция который должный быть...</td>\n",
       "      <td>[(главный проблема сша, 9.0), (проблема измене...</td>\n",
       "      <td>[(байден американский, 0.22951372473084455), (...</td>\n",
       "      <td>[(американский президент, 0.000585938267381143...</td>\n",
       "      <td>[глобальное потепление, изменение климата, Бай...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.vesti.ru/article/2635786</td>\n",
       "      <td>[Томас Бенфилд, вакцинация, коронавирус, симпт...</td>\n",
       "      <td>[общество, медицина, болезнь, Дания, вакцинаци...</td>\n",
       "      <td>Инфекционист Томас Бенфилд и иммунолог Руне Ха...</td>\n",
       "      <td>инфекционист томас бенфилд и иммунолог руно ха...</td>\n",
       "      <td>[(инфекционист томас бенфилд, 9.0), (это крупн...</td>\n",
       "      <td>[(инфекционист томас бенфилд, 0.48051180797354...</td>\n",
       "      <td>[(томас бенфилд, 0.002684672417102007), (инфек...</td>\n",
       "      <td>[снижение риска заболевания, коронавирус, Дани...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        source  \\\n",
       "0         https://www.vesti.ru/article/2635384   \n",
       "1  https://www.vesti.ru/hitech/article/2635408   \n",
       "2         https://www.vesti.ru/article/2635790   \n",
       "3         https://www.vesti.ru/article/2635756   \n",
       "4         https://www.vesti.ru/article/2635786   \n",
       "\n",
       "                                         my_keywords  \\\n",
       "0  [Милош Земан, президент, реанимация, состояние...   \n",
       "1  [деньги, мошенники, атака, жертва мошенников, ...   \n",
       "2  [Аэрофлот, Пхукет, Таиланд, регулярное авиасоо...   \n",
       "3  [Глазго, президент, Байден, климатическая конф...   \n",
       "4  [Томас Бенфилд, вакцинация, коронавирус, симпт...   \n",
       "\n",
       "                                keywords_from_source  \\\n",
       "0  [общество, в мире, политика, президент, Чехия,...   \n",
       "1  [общество, происшествия, технологии, обман, об...   \n",
       "2  [экономика, Таиланд, Аэрофлот, Бангкок, Пхукет...   \n",
       "3  [общество, в мире, встреча, климат, США Америк...   \n",
       "4  [общество, медицина, болезнь, Дания, вакцинаци...   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  Медицинский консилиум оценит в пятницу состоян...   \n",
       "1  Нередко жертвами мошенников становятся те кто ...   \n",
       "2  Первый за последние месяцев самолет компании А...   \n",
       "3  Климатическая конференция которая должна была ...   \n",
       "4  Инфекционист Томас Бенфилд и иммунолог Руне Ха...   \n",
       "\n",
       "                                      normalize_text  \\\n",
       "0  медицинский консилиум оценить в пятница состоя...   \n",
       "1  нередко жертва мошенник становиться тот кто ра...   \n",
       "2  один за последний месяц самолёт компания аэроф...   \n",
       "3  климатический конференция который должный быть...   \n",
       "4  инфекционист томас бенфилд и иммунолог руно ха...   \n",
       "\n",
       "                                                rake  \\\n",
       "0  [(медицинский консилиум оценить, 9.0), (состоя...   \n",
       "1  [(якобы проверить действительно, 9.0), (сайт м...   \n",
       "2  [(выполнять регулярный авиарейс, 9.0), (прохож...   \n",
       "3  [(главный проблема сша, 9.0), (проблема измене...   \n",
       "4  [(инфекционист томас бенфилд, 9.0), (это крупн...   \n",
       "\n",
       "                                            textrank  \\\n",
       "0  [(земан, 0.1923731054306261), (состояние здоро...   \n",
       "1  [(атака, 0.21545317808030248), (жертва мошенни...   \n",
       "2  [(таиланд, 0.22954872965166995), (регулярный, ...   \n",
       "3  [(байден американский, 0.22951372473084455), (...   \n",
       "4  [(инфекционист томас бенфилд, 0.48051180797354...   \n",
       "\n",
       "                                                yake  \\\n",
       "0  [(состояние здоровье, 0.0005116003525649392), ...   \n",
       "1  [(виктор чебышев, 0.002687649831204984), (касп...   \n",
       "2  [(составлять миллион, 0.008638577397523413), (...   \n",
       "3  [(американский президент, 0.000585938267381143...   \n",
       "4  [(томас бенфилд, 0.002684672417102007), (инфек...   \n",
       "\n",
       "                                   combined_keywords  \n",
       "0  [правительство, состояние здоровья, президент,...  \n",
       "1  [мошенник, обман, технологии, жертва мошеннико...  \n",
       "2  [Аэрофлот, Паттайя, Пхукет, Таиланд, экономика...  \n",
       "3  [глобальное потепление, изменение климата, Бай...  \n",
       "4  [снижение риска заболевания, коронавирус, Дани...  "
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Можно объединить все ключевые слова в один список и сделать морфологические шаблоны через pos-tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forfor(a):\n",
    "    return [item for sublist in a for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keywords = forfor(list(main_df.combined_keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "for element in all_keywords:\n",
    "    sample = []\n",
    "    element2 = element.split()\n",
    "    for el in element2:\n",
    "        pos_element = morph.parse(el)\n",
    "        sample.append(pos_element[0].tag.POS)\n",
    "    d[element] = sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_elements = []\n",
    "for k in list(d.values()):\n",
    "    if k not in unique_elements:\n",
    "        unique_elements.append(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Мы получили список шаблонов на основании части речи:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['NOUN'],\n",
       " ['NOUN', 'NOUN'],\n",
       " ['ADJF', 'NOUN'],\n",
       " ['PRTF', 'NOUN'],\n",
       " ['NOUN', 'NOUN', 'NOUN'],\n",
       " ['ADJF', 'ADJF', 'NOUN'],\n",
       " ['NOUN', 'PREP', 'NOUN'],\n",
       " ['ADJF', 'NOUN', 'NOUN'],\n",
       " ['PRCL', 'NOUN', 'NOUN']]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Теперь напишем функцию, которая будет размечать ключевые слова, полученные с помощью Rake, TextRank и Yake:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_keywords(keywords):\n",
    "    d = {}\n",
    "    for el in keywords:\n",
    "        element = list(el)[0]\n",
    "        sample = []\n",
    "        for el2 in element.split():\n",
    "            pos_element = morph.parse(el2)\n",
    "            sample.append(pos_element[0].tag.POS)\n",
    "        d[element] = sample\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Воспользуемся функцией, которая будет фильтровать ключевые слова в зависимости от шаблонов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_keywords(keywords_dict, unique_elements):\n",
    "    keywords_sorted = []\n",
    "    for k, v in keywords_dict.items():\n",
    "        if v in unique_elements:\n",
    "            keywords_sorted.append(k)\n",
    "    return keywords_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df['sorted_pos_rake'] = main_df['rake'].apply(lambda x: sorted_keywords(get_pos_keywords(x), unique_elements))\n",
    "main_df['sorted_pos_textrank'] = main_df['textrank'].apply(lambda x: sorted_keywords(get_pos_keywords(x), unique_elements))\n",
    "main_df['sorted_pos_yake'] = main_df['yake'].apply(lambda x: sorted_keywords(get_pos_keywords(x), unique_elements))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Теперь нужно оценить пересечение с нашими эталонными ключевыми словами. Поскольку некоторые эталонные ключевые слова используются не в начальной форме, то, возможно имеет смысл привести их к начальной форме..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma_keywords(keywords):\n",
    "    keywords_lemmatized = []\n",
    "    for el in keywords:\n",
    "        element = normalize_text(el)\n",
    "        keywords_lemmatized.append(element)\n",
    "    return keywords_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df['lemmatized_keywords'] = main_df['combined_keywords'].apply(lambda x:lemma_keywords(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Функция для подсчета метрик"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>my_keywords</th>\n",
       "      <th>keywords_from_source</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>normalize_text</th>\n",
       "      <th>rake</th>\n",
       "      <th>textrank</th>\n",
       "      <th>yake</th>\n",
       "      <th>combined_keywords</th>\n",
       "      <th>sorted_pos_rake</th>\n",
       "      <th>sorted_pos_textrank</th>\n",
       "      <th>sorted_pos_yake</th>\n",
       "      <th>lemmatized_keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.vesti.ru/article/2635384</td>\n",
       "      <td>[Милош Земан, президент, реанимация, состояние...</td>\n",
       "      <td>[общество, в мире, политика, президент, Чехия,...</td>\n",
       "      <td>Медицинский консилиум оценит в пятницу состоян...</td>\n",
       "      <td>медицинский консилиум оценить в пятница состоя...</td>\n",
       "      <td>[(медицинский консилиум оценить, 9.0), (состоя...</td>\n",
       "      <td>[(земан, 0.1923731054306261), (состояние здоро...</td>\n",
       "      <td>[(состояние здоровье, 0.0005116003525649392), ...</td>\n",
       "      <td>[правительство, состояние здоровья, президент,...</td>\n",
       "      <td>[пониженный артериальный давление, улучшение с...</td>\n",
       "      <td>[земан, состояние здоровье президент, милош зе...</td>\n",
       "      <td>[состояние здоровье, военный госпиталь, госпит...</td>\n",
       "      <td>[правительство, состояние здоровье, президент,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.vesti.ru/hitech/article/2635408</td>\n",
       "      <td>[деньги, мошенники, атака, жертва мошенников, ...</td>\n",
       "      <td>[общество, происшествия, технологии, обман, об...</td>\n",
       "      <td>Нередко жертвами мошенников становятся те кто ...</td>\n",
       "      <td>нередко жертва мошенник становиться тот кто ра...</td>\n",
       "      <td>[(якобы проверить действительно, 9.0), (сайт м...</td>\n",
       "      <td>[(атака, 0.21545317808030248), (жертва мошенни...</td>\n",
       "      <td>[(виктор чебышев, 0.002687649831204984), (касп...</td>\n",
       "      <td>[мошенник, обман, технологии, жертва мошеннико...</td>\n",
       "      <td>[слово виктор чебышев, интернет объявление, бе...</td>\n",
       "      <td>[атака, жертва мошенник, сообщение, этап, мочь...</td>\n",
       "      <td>[виктор чебышев, касперский виктор, чебышев зл...</td>\n",
       "      <td>[мошенник, обман, технология, жертва мошенник,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.vesti.ru/article/2635790</td>\n",
       "      <td>[Аэрофлот, Пхукет, Таиланд, регулярное авиасоо...</td>\n",
       "      <td>[экономика, Таиланд, Аэрофлот, Бангкок, Пхукет...</td>\n",
       "      <td>Первый за последние месяцев самолет компании А...</td>\n",
       "      <td>один за последний месяц самолёт компания аэроф...</td>\n",
       "      <td>[(выполнять регулярный авиарейс, 9.0), (прохож...</td>\n",
       "      <td>[(таиланд, 0.22954872965166995), (регулярный, ...</td>\n",
       "      <td>[(составлять миллион, 0.008638577397523413), (...</td>\n",
       "      <td>[Аэрофлот, Паттайя, Пхукет, Таиланд, экономика...</td>\n",
       "      <td>[страна январь год, обязательный нахождение, п...</td>\n",
       "      <td>[таиланд, пхукет, аэрофлот, самолёт, миллион ч...</td>\n",
       "      <td>[условие бангкок, последний месяц, водяной арка]</td>\n",
       "      <td>[аэрофлот, паттайя, пхукет, таиланд, экономика...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.vesti.ru/article/2635756</td>\n",
       "      <td>[Глазго, президент, Байден, климатическая конф...</td>\n",
       "      <td>[общество, в мире, встреча, климат, США Америк...</td>\n",
       "      <td>Климатическая конференция которая должна была ...</td>\n",
       "      <td>климатический конференция который должный быть...</td>\n",
       "      <td>[(главный проблема сша, 9.0), (проблема измене...</td>\n",
       "      <td>[(байден американский, 0.22951372473084455), (...</td>\n",
       "      <td>[(американский президент, 0.000585938267381143...</td>\n",
       "      <td>[глобальное потепление, изменение климата, Бай...</td>\n",
       "      <td>[главный проблема сша, проблема изменение клим...</td>\n",
       "      <td>[президент сша, глазго, администрация, весь ст...</td>\n",
       "      <td>[американский президент, изменение климат, угл...</td>\n",
       "      <td>[глобальный потепление, изменение климат, байд...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.vesti.ru/article/2635786</td>\n",
       "      <td>[Томас Бенфилд, вакцинация, коронавирус, симпт...</td>\n",
       "      <td>[общество, медицина, болезнь, Дания, вакцинаци...</td>\n",
       "      <td>Инфекционист Томас Бенфилд и иммунолог Руне Ха...</td>\n",
       "      <td>инфекционист томас бенфилд и иммунолог руно ха...</td>\n",
       "      <td>[(инфекционист томас бенфилд, 9.0), (это крупн...</td>\n",
       "      <td>[(инфекционист томас бенфилд, 0.48051180797354...</td>\n",
       "      <td>[(томас бенфилд, 0.002684672417102007), (инфек...</td>\n",
       "      <td>[снижение риска заболевания, коронавирус, Дани...</td>\n",
       "      <td>[инфекционист томас бенфилд, слово томас бенфи...</td>\n",
       "      <td>[инфекционист томас бенфилд, слово, чаща]</td>\n",
       "      <td>[томас бенфилд, инфекционист томас, страна евр...</td>\n",
       "      <td>[снижение риска заболевание, коронавирус, дани...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        source  \\\n",
       "0         https://www.vesti.ru/article/2635384   \n",
       "1  https://www.vesti.ru/hitech/article/2635408   \n",
       "2         https://www.vesti.ru/article/2635790   \n",
       "3         https://www.vesti.ru/article/2635756   \n",
       "4         https://www.vesti.ru/article/2635786   \n",
       "\n",
       "                                         my_keywords  \\\n",
       "0  [Милош Земан, президент, реанимация, состояние...   \n",
       "1  [деньги, мошенники, атака, жертва мошенников, ...   \n",
       "2  [Аэрофлот, Пхукет, Таиланд, регулярное авиасоо...   \n",
       "3  [Глазго, президент, Байден, климатическая конф...   \n",
       "4  [Томас Бенфилд, вакцинация, коронавирус, симпт...   \n",
       "\n",
       "                                keywords_from_source  \\\n",
       "0  [общество, в мире, политика, президент, Чехия,...   \n",
       "1  [общество, происшествия, технологии, обман, об...   \n",
       "2  [экономика, Таиланд, Аэрофлот, Бангкок, Пхукет...   \n",
       "3  [общество, в мире, встреча, климат, США Америк...   \n",
       "4  [общество, медицина, болезнь, Дания, вакцинаци...   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  Медицинский консилиум оценит в пятницу состоян...   \n",
       "1  Нередко жертвами мошенников становятся те кто ...   \n",
       "2  Первый за последние месяцев самолет компании А...   \n",
       "3  Климатическая конференция которая должна была ...   \n",
       "4  Инфекционист Томас Бенфилд и иммунолог Руне Ха...   \n",
       "\n",
       "                                      normalize_text  \\\n",
       "0  медицинский консилиум оценить в пятница состоя...   \n",
       "1  нередко жертва мошенник становиться тот кто ра...   \n",
       "2  один за последний месяц самолёт компания аэроф...   \n",
       "3  климатический конференция который должный быть...   \n",
       "4  инфекционист томас бенфилд и иммунолог руно ха...   \n",
       "\n",
       "                                                rake  \\\n",
       "0  [(медицинский консилиум оценить, 9.0), (состоя...   \n",
       "1  [(якобы проверить действительно, 9.0), (сайт м...   \n",
       "2  [(выполнять регулярный авиарейс, 9.0), (прохож...   \n",
       "3  [(главный проблема сша, 9.0), (проблема измене...   \n",
       "4  [(инфекционист томас бенфилд, 9.0), (это крупн...   \n",
       "\n",
       "                                            textrank  \\\n",
       "0  [(земан, 0.1923731054306261), (состояние здоро...   \n",
       "1  [(атака, 0.21545317808030248), (жертва мошенни...   \n",
       "2  [(таиланд, 0.22954872965166995), (регулярный, ...   \n",
       "3  [(байден американский, 0.22951372473084455), (...   \n",
       "4  [(инфекционист томас бенфилд, 0.48051180797354...   \n",
       "\n",
       "                                                yake  \\\n",
       "0  [(состояние здоровье, 0.0005116003525649392), ...   \n",
       "1  [(виктор чебышев, 0.002687649831204984), (касп...   \n",
       "2  [(составлять миллион, 0.008638577397523413), (...   \n",
       "3  [(американский президент, 0.000585938267381143...   \n",
       "4  [(томас бенфилд, 0.002684672417102007), (инфек...   \n",
       "\n",
       "                                   combined_keywords  \\\n",
       "0  [правительство, состояние здоровья, президент,...   \n",
       "1  [мошенник, обман, технологии, жертва мошеннико...   \n",
       "2  [Аэрофлот, Паттайя, Пхукет, Таиланд, экономика...   \n",
       "3  [глобальное потепление, изменение климата, Бай...   \n",
       "4  [снижение риска заболевания, коронавирус, Дани...   \n",
       "\n",
       "                                     sorted_pos_rake  \\\n",
       "0  [пониженный артериальный давление, улучшение с...   \n",
       "1  [слово виктор чебышев, интернет объявление, бе...   \n",
       "2  [страна январь год, обязательный нахождение, п...   \n",
       "3  [главный проблема сша, проблема изменение клим...   \n",
       "4  [инфекционист томас бенфилд, слово томас бенфи...   \n",
       "\n",
       "                                 sorted_pos_textrank  \\\n",
       "0  [земан, состояние здоровье президент, милош зе...   \n",
       "1  [атака, жертва мошенник, сообщение, этап, мочь...   \n",
       "2  [таиланд, пхукет, аэрофлот, самолёт, миллион ч...   \n",
       "3  [президент сша, глазго, администрация, весь ст...   \n",
       "4          [инфекционист томас бенфилд, слово, чаща]   \n",
       "\n",
       "                                     sorted_pos_yake  \\\n",
       "0  [состояние здоровье, военный госпиталь, госпит...   \n",
       "1  [виктор чебышев, касперский виктор, чебышев зл...   \n",
       "2   [условие бангкок, последний месяц, водяной арка]   \n",
       "3  [американский президент, изменение климат, угл...   \n",
       "4  [томас бенфилд, инфекционист томас, страна евр...   \n",
       "\n",
       "                                 lemmatized_keywords  \n",
       "0  [правительство, состояние здоровье, президент,...  \n",
       "1  [мошенник, обман, технология, жертва мошенник,...  \n",
       "2  [аэрофлот, паттайя, пхукет, таиланд, экономика...  \n",
       "3  [глобальный потепление, изменение климат, байд...  \n",
       "4  [снижение риска заболевание, коронавирус, дани...  "
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intersection(main_keywords, other_keywords):\n",
    "    all_keywords = []\n",
    "    for el in other_keywords:\n",
    "        if el in main_keywords:\n",
    "            all_keywords.append(el)\n",
    "    precision = len(all_keywords) / len(other_keywords)\n",
    "    recall = len(all_keywords) / len(main_keywords)\n",
    "    if precision == 0 and recall == 0:\n",
    "        f_score = 0.0\n",
    "    else:\n",
    "        f_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return precision, recall, f_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_metrics(list1, list2):\n",
    "    metrics_precision = []\n",
    "    metrics_recall = []\n",
    "    metrics_fscore = []\n",
    "    for i in range(len(list1)):\n",
    "        metric = get_intersection(list1[i], list2[i])\n",
    "        metrics_precision.append(metric[0])\n",
    "        metrics_recall.append(metric[1])\n",
    "        metrics_fscore.append(metric[2])\n",
    "    return metrics_precision, metrics_recall, metrics_fscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Посчитаем метрики для каждого текста с учетом шаблонов:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df['rake_precision'] = get_all_metrics(list(main_df.lemmatized_keywords), list(main_df.sorted_pos_rake))[0]\n",
    "main_df['rake_recall'] = get_all_metrics(list(main_df.lemmatized_keywords), list(main_df.sorted_pos_rake))[1]\n",
    "main_df['rake_fscore'] = get_all_metrics(list(main_df.lemmatized_keywords), list(main_df.sorted_pos_rake))[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df['textrank_precision'] = get_all_metrics(list(main_df.lemmatized_keywords), list(main_df.sorted_pos_textrank))[0]\n",
    "main_df['textrank_recall'] = get_all_metrics(list(main_df.lemmatized_keywords), list(main_df.sorted_pos_textrank))[1]\n",
    "main_df['textrank_fscore'] = get_all_metrics(list(main_df.lemmatized_keywords), list(main_df.sorted_pos_textrank))[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df['yake_precision'] = get_all_metrics(list(main_df.lemmatized_keywords), list(main_df.sorted_pos_yake))[0]\n",
    "main_df['yake_recall'] = get_all_metrics(list(main_df.lemmatized_keywords), list(main_df.sorted_pos_yake))[1]\n",
    "main_df['yake_fscore'] = get_all_metrics(list(main_df.lemmatized_keywords), list(main_df.sorted_pos_yake))[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Посчитаем метрики без учета шаблонов:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Функция для извлечения ключевых слов без значений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_keywords(keywords):\n",
    "    keywordsClean = []\n",
    "    for el in keywords:\n",
    "        keywordsClean.append(el[0])\n",
    "    return keywordsClean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df['clean_keywords_rake'] = main_df['rake'].apply(lambda x: clean_keywords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df['clean_keywords_textrank'] = main_df['textrank'].apply(lambda x: clean_keywords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df['clean_keywords_yake'] = main_df['yake'].apply(lambda x: clean_keywords(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df['rake_precision_without_patterns'] = get_all_metrics(list(main_df.lemmatized_keywords), list(main_df.clean_keywords_rake))[0]\n",
    "main_df['rake_recall_without_patterns'] = get_all_metrics(list(main_df.lemmatized_keywords), list(main_df.clean_keywords_rake))[1]\n",
    "main_df['rake_fscore_without_patterns'] = get_all_metrics(list(main_df.lemmatized_keywords), list(main_df.clean_keywords_rake))[2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df['textrank_precision_without_patterns'] = get_all_metrics(list(main_df.lemmatized_keywords), list(main_df.clean_keywords_textrank))[0]\n",
    "main_df['textrank_recall_without_patterns'] = get_all_metrics(list(main_df.lemmatized_keywords), list(main_df.clean_keywords_textrank))[1]\n",
    "main_df['textrank_fscore_without_patterns'] = get_all_metrics(list(main_df.lemmatized_keywords), list(main_df.clean_keywords_textrank))[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df['yake_precision_without_patterns'] = get_all_metrics(list(main_df.lemmatized_keywords), list(main_df.clean_keywords_yake))[0]\n",
    "main_df['yake_recall_without_patterns'] = get_all_metrics(list(main_df.lemmatized_keywords), list(main_df.clean_keywords_yake))[1]\n",
    "main_df['yake_fscore_without_patterns'] = get_all_metrics(list(main_df.lemmatized_keywords), list(main_df.clean_keywords_yake))[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "rake_scores = main_df[['rake_precision', 'rake_recall', 'rake_fscore', 'rake_precision_without_patterns', 'rake_recall_without_patterns', 'rake_fscore_without_patterns']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rake_precision</th>\n",
       "      <th>rake_recall</th>\n",
       "      <th>rake_fscore</th>\n",
       "      <th>rake_precision_without_patterns</th>\n",
       "      <th>rake_recall_without_patterns</th>\n",
       "      <th>rake_fscore_without_patterns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.081081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.097561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.137931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.113208</td>\n",
       "      <td>0.042857</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.074074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.064516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.025316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.040816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.034884</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.060606</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rake_precision  rake_recall  rake_fscore  rake_precision_without_patterns  \\\n",
       "0        0.090909     0.300000     0.139535                         0.046875   \n",
       "1        0.153846     0.153846     0.153846                         0.071429   \n",
       "2        0.153846     0.200000     0.173913                         0.105263   \n",
       "3        0.071429     0.272727     0.113208                         0.042857   \n",
       "4        0.071429     0.111111     0.086957                         0.045455   \n",
       "5        0.200000     0.181818     0.190476                         0.060606   \n",
       "6        0.029412     0.100000     0.045455                         0.014493   \n",
       "7        0.066667     0.100000     0.080000                         0.025641   \n",
       "8        0.078947     0.230769     0.117647                         0.034884   \n",
       "\n",
       "   rake_recall_without_patterns  rake_fscore_without_patterns  \n",
       "0                      0.300000                      0.081081  \n",
       "1                      0.153846                      0.097561  \n",
       "2                      0.200000                      0.137931  \n",
       "3                      0.272727                      0.074074  \n",
       "4                      0.111111                      0.064516  \n",
       "5                      0.181818                      0.090909  \n",
       "6                      0.100000                      0.025316  \n",
       "7                      0.100000                      0.040816  \n",
       "8                      0.230769                      0.060606  "
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rake_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rake_precision</th>\n",
       "      <th>rake_recall</th>\n",
       "      <th>rake_fscore</th>\n",
       "      <th>rake_precision_without_patterns</th>\n",
       "      <th>rake_recall_without_patterns</th>\n",
       "      <th>rake_fscore_without_patterns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.101832</td>\n",
       "      <td>0.183364</td>\n",
       "      <td>0.122337</td>\n",
       "      <td>0.049722</td>\n",
       "      <td>0.183364</td>\n",
       "      <td>0.074757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.054796</td>\n",
       "      <td>0.074252</td>\n",
       "      <td>0.046974</td>\n",
       "      <td>0.026900</td>\n",
       "      <td>0.074252</td>\n",
       "      <td>0.033003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.025316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.034884</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.060606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.074074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.137931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       rake_precision  rake_recall  rake_fscore  \\\n",
       "count        9.000000     9.000000     9.000000   \n",
       "mean         0.101832     0.183364     0.122337   \n",
       "std          0.054796     0.074252     0.046974   \n",
       "min          0.029412     0.100000     0.045455   \n",
       "25%          0.071429     0.111111     0.086957   \n",
       "50%          0.078947     0.181818     0.117647   \n",
       "75%          0.153846     0.230769     0.153846   \n",
       "max          0.200000     0.300000     0.190476   \n",
       "\n",
       "       rake_precision_without_patterns  rake_recall_without_patterns  \\\n",
       "count                         9.000000                      9.000000   \n",
       "mean                          0.049722                      0.183364   \n",
       "std                           0.026900                      0.074252   \n",
       "min                           0.014493                      0.100000   \n",
       "25%                           0.034884                      0.111111   \n",
       "50%                           0.045455                      0.181818   \n",
       "75%                           0.060606                      0.230769   \n",
       "max                           0.105263                      0.300000   \n",
       "\n",
       "       rake_fscore_without_patterns  \n",
       "count                      9.000000  \n",
       "mean                       0.074757  \n",
       "std                        0.033003  \n",
       "min                        0.025316  \n",
       "25%                        0.060606  \n",
       "50%                        0.074074  \n",
       "75%                        0.090909  \n",
       "max                        0.137931  "
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rake_scores.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "textrank_scores = main_df[['textrank_precision', 'textrank_recall', 'textrank_fscore', 'textrank_precision_without_patterns', 'textrank_recall_without_patterns', 'textrank_fscore_without_patterns']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textrank_precision</th>\n",
       "      <th>textrank_recall</th>\n",
       "      <th>textrank_fscore</th>\n",
       "      <th>textrank_precision_without_patterns</th>\n",
       "      <th>textrank_recall_without_patterns</th>\n",
       "      <th>textrank_fscore_without_patterns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.085106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.206897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.045455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.148148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   textrank_precision  textrank_recall  textrank_fscore  \\\n",
       "0            0.076923         0.200000         0.111111   \n",
       "1            0.272727         0.230769         0.250000   \n",
       "2            0.272727         0.300000         0.285714   \n",
       "3            0.047619         0.090909         0.062500   \n",
       "4            0.000000         0.000000         0.000000   \n",
       "5            0.250000         0.181818         0.210526   \n",
       "6            0.117647         0.200000         0.148148   \n",
       "7            0.200000         0.100000         0.133333   \n",
       "8            0.000000         0.000000         0.000000   \n",
       "\n",
       "   textrank_precision_without_patterns  textrank_recall_without_patterns  \\\n",
       "0                             0.054054                          0.200000   \n",
       "1                             0.187500                          0.230769   \n",
       "2                             0.200000                          0.300000   \n",
       "3                             0.030303                          0.090909   \n",
       "4                             0.000000                          0.000000   \n",
       "5                             0.125000                          0.181818   \n",
       "6                             0.093750                          0.300000   \n",
       "7                             0.050000                          0.100000   \n",
       "8                             0.000000                          0.000000   \n",
       "\n",
       "   textrank_fscore_without_patterns  \n",
       "0                          0.085106  \n",
       "1                          0.206897  \n",
       "2                          0.240000  \n",
       "3                          0.045455  \n",
       "4                          0.000000  \n",
       "5                          0.148148  \n",
       "6                          0.142857  \n",
       "7                          0.066667  \n",
       "8                          0.000000  "
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textrank_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textrank_precision</th>\n",
       "      <th>textrank_recall</th>\n",
       "      <th>textrank_fscore</th>\n",
       "      <th>textrank_precision_without_patterns</th>\n",
       "      <th>textrank_recall_without_patterns</th>\n",
       "      <th>textrank_fscore_without_patterns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.137516</td>\n",
       "      <td>0.144833</td>\n",
       "      <td>0.133481</td>\n",
       "      <td>0.082290</td>\n",
       "      <td>0.155944</td>\n",
       "      <td>0.103903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.113518</td>\n",
       "      <td>0.103554</td>\n",
       "      <td>0.102516</td>\n",
       "      <td>0.074972</td>\n",
       "      <td>0.114951</td>\n",
       "      <td>0.086134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.045455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.085106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.148148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.240000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textrank_precision  textrank_recall  textrank_fscore  \\\n",
       "count            9.000000         9.000000         9.000000   \n",
       "mean             0.137516         0.144833         0.133481   \n",
       "std              0.113518         0.103554         0.102516   \n",
       "min              0.000000         0.000000         0.000000   \n",
       "25%              0.047619         0.090909         0.062500   \n",
       "50%              0.117647         0.181818         0.133333   \n",
       "75%              0.250000         0.200000         0.210526   \n",
       "max              0.272727         0.300000         0.285714   \n",
       "\n",
       "       textrank_precision_without_patterns  textrank_recall_without_patterns  \\\n",
       "count                             9.000000                          9.000000   \n",
       "mean                              0.082290                          0.155944   \n",
       "std                               0.074972                          0.114951   \n",
       "min                               0.000000                          0.000000   \n",
       "25%                               0.030303                          0.090909   \n",
       "50%                               0.054054                          0.181818   \n",
       "75%                               0.125000                          0.230769   \n",
       "max                               0.200000                          0.300000   \n",
       "\n",
       "       textrank_fscore_without_patterns  \n",
       "count                          9.000000  \n",
       "mean                           0.103903  \n",
       "std                            0.086134  \n",
       "min                            0.000000  \n",
       "25%                            0.045455  \n",
       "50%                            0.085106  \n",
       "75%                            0.148148  \n",
       "max                            0.240000  "
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textrank_scores.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "yake_scores = main_df[['yake_precision', 'yake_recall', 'yake_fscore', 'yake_precision_without_patterns', 'yake_recall_without_patterns', 'yake_fscore_without_patterns']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yake_precision</th>\n",
       "      <th>yake_recall</th>\n",
       "      <th>yake_fscore</th>\n",
       "      <th>yake_precision_without_patterns</th>\n",
       "      <th>yake_recall_without_patterns</th>\n",
       "      <th>yake_fscore_without_patterns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.095238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.105263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.173913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   yake_precision  yake_recall  yake_fscore  yake_precision_without_patterns  \\\n",
       "0        0.200000     0.200000     0.200000                              0.2   \n",
       "1        0.000000     0.000000     0.000000                              0.0   \n",
       "2        0.000000     0.000000     0.000000                              0.0   \n",
       "3        0.125000     0.090909     0.105263                              0.1   \n",
       "4        0.166667     0.111111     0.133333                              0.1   \n",
       "5        0.000000     0.000000     0.000000                              0.0   \n",
       "6        0.333333     0.100000     0.153846                              0.2   \n",
       "7        0.333333     0.100000     0.153846                              0.1   \n",
       "8        0.285714     0.153846     0.200000                              0.2   \n",
       "\n",
       "   yake_recall_without_patterns  yake_fscore_without_patterns  \n",
       "0                      0.200000                      0.200000  \n",
       "1                      0.000000                      0.000000  \n",
       "2                      0.000000                      0.000000  \n",
       "3                      0.090909                      0.095238  \n",
       "4                      0.111111                      0.105263  \n",
       "5                      0.000000                      0.000000  \n",
       "6                      0.200000                      0.200000  \n",
       "7                      0.100000                      0.100000  \n",
       "8                      0.153846                      0.173913  "
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yake_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yake_precision</th>\n",
       "      <th>yake_recall</th>\n",
       "      <th>yake_fscore</th>\n",
       "      <th>yake_precision_without_patterns</th>\n",
       "      <th>yake_recall_without_patterns</th>\n",
       "      <th>yake_fscore_without_patterns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.160450</td>\n",
       "      <td>0.083985</td>\n",
       "      <td>0.105143</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.095096</td>\n",
       "      <td>0.097157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.139446</td>\n",
       "      <td>0.071412</td>\n",
       "      <td>0.084187</td>\n",
       "      <td>0.086603</td>\n",
       "      <td>0.081309</td>\n",
       "      <td>0.083247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.173913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       yake_precision  yake_recall  yake_fscore  \\\n",
       "count        9.000000     9.000000     9.000000   \n",
       "mean         0.160450     0.083985     0.105143   \n",
       "std          0.139446     0.071412     0.084187   \n",
       "min          0.000000     0.000000     0.000000   \n",
       "25%          0.000000     0.000000     0.000000   \n",
       "50%          0.166667     0.100000     0.133333   \n",
       "75%          0.285714     0.111111     0.153846   \n",
       "max          0.333333     0.200000     0.200000   \n",
       "\n",
       "       yake_precision_without_patterns  yake_recall_without_patterns  \\\n",
       "count                         9.000000                      9.000000   \n",
       "mean                          0.100000                      0.095096   \n",
       "std                           0.086603                      0.081309   \n",
       "min                           0.000000                      0.000000   \n",
       "25%                           0.000000                      0.000000   \n",
       "50%                           0.100000                      0.100000   \n",
       "75%                           0.200000                      0.153846   \n",
       "max                           0.200000                      0.200000   \n",
       "\n",
       "       yake_fscore_without_patterns  \n",
       "count                      9.000000  \n",
       "mean                       0.097157  \n",
       "std                        0.083247  \n",
       "min                        0.000000  \n",
       "25%                        0.000000  \n",
       "50%                        0.100000  \n",
       "75%                        0.173913  \n",
       "max                        0.200000  "
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yake_scores.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Главные выводы и ошибки: \n",
    "1. ключевые слова разных методов пересекаются редко.\n",
    "2. выделяются глаголы, которые в русском языке не используются как ключевые слова. Возможно, решением данной проблемы будет являться перевод из одной части речи в другую, то есть из глагола в существительное.\n",
    "3. ключевые слова методов и эталонные ключевые слова не пересекаются из несоответствия длины данных ключевых слов. Например, в эталоне выделяется \"Томас Бенфилд\", а в ключевых словах, полученных с помощью методов содержит почти такое же, но \"инфекционист Томас Бенфилд\". Из-за неполного соответствия данный пример уже не включается в пересечение между эталонными ключевыми словами и ключевыми словами, полученными с помощью методов. Эту проблему можно решить: проверять ключевые слова не на полное соответствие, а на включение одного множества в другое (как в данном примере) и не выкидывать этот пример, когда делаем подсчета метрик.\n",
    "4. Метрики получились не самыми лучшими."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
